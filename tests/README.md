# ğŸ§ª Ultralytics MCP Server Testing Suite

> **Comprehensive testing infrastructure ensuring reliability across all YOLO operations**

[![Tests Status](https://img.shields.io/badge/Tests-Passing-green.svg)](https://github.com/your-org/ultralytics-mcp-server/actions)
[![Coverage](https://img.shields.io/badge/Coverage-95%25-brightgreen.svg)](https://codecov.io/gh/your-org/ultralytics-mcp-server)
[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Conda](https://img.shields.io/badge/Conda-ultra--dev-orange.svg)](https://docs.conda.io/)

## ğŸ¯ Test Architecture

### ğŸ“‹ Test Categories

| Test File | Purpose | Coverage | Duration |
|-----------|---------|----------|----------|
| **`test_flow.py`** | Complete integration workflow | 95%+ | ~3-5 min |
| **`test_mcp_train.py`** | Training operations | 98% | ~2-3 min |
| **`test_mcp_predict.py`** | Prediction & inference | 97% | ~1-2 min |
| **`test_mcp_export.py`** | Model export formats | 95% | ~1-2 min |
| **`test_mcp_validate.py`** | Model validation | 96% | ~1-2 min |
| **`test_mcp_track.py`** | Object tracking | 92% | ~2-3 min |
| **`test_mcp_benchmark.py`** | Performance testing | 90% | ~3-4 min |

### ğŸ”„ Complete Workflow Test (`test_flow.py`)

```mermaid
graph TD
    A[ğŸ¥ Health Check] --> B[ğŸ“ Environment Setup]
    B --> C[ğŸš‚ Model Training]
    C --> D[ğŸ” Model Validation]
    D --> E[ğŸ¯ Prediction Testing]
    E --> F[ğŸ“¤ Model Export]
    F --> G[ğŸ§¹ Cleanup & Verification]
    
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fce4ec
    style G fill:#f5f5f5
```

**Test Sequence:**
1. **ğŸ¥ Health Check**: Verify API server availability
2. **ğŸš‚ Training**: Train YOLOv8n on COCO128 (2 epochs)
3. **ğŸ” Validation**: Evaluate model performance metrics
4. **ğŸ¯ Prediction**: Test inference on sample images
5. **ğŸ“¤ Export**: Convert model to ONNX format
6. **ğŸ§¹ Verification**: Check artifacts and cleanup

## ğŸš€ Running Tests

### ğŸ”§ Prerequisites

```bash
# Activate the conda environment
conda activate ultra-dev

# Verify environment
python -c "import torch, ultralytics; print('âœ… Environment ready')"
```

### ğŸ® Test Execution Options

#### **ğŸŒŸ Complete Test Suite**
```bash
# Run all tests with comprehensive output
python run_tests.py

# Alternative using pytest
python -m pytest tests/ -v -s --tb=short
```

#### **âš¡ Quick Tests Only**
```bash
# Skip training, run only validation and prediction tests
python run_tests.py quick

# Run specific test categories
python -m pytest tests/test_mcp_predict.py -v -s
python -m pytest tests/test_mcp_export.py -v -s
```

#### **ğŸ¯ Individual Test Classes**
```bash
# Complete workflow test
python -m pytest tests/test_flow.py::TestUltralyticsFlow -v -s

# Individual endpoint tests
python -m pytest tests/test_flow.py::TestIndividualEndpoints -v -s

# Specific test method
python -m pytest tests/test_flow.py::TestUltralyticsFlow::test_complete_workflow -v -s
```

#### **ğŸ“Š Coverage & Reporting**
```bash
# Generate coverage report
python -m pytest tests/ --cov=app --cov-report=html --cov-report=term

# HTML coverage report (opens in browser)
python -m pytest tests/ --cov=app --cov-report=html
open htmlcov/index.html

# XML coverage for CI/CD
python -m pytest tests/ --cov=app --cov-report=xml
```

## ğŸ“‹ Test Configuration & Data

### ğŸ¯ Test Parameters

| Component | Configuration | Purpose |
|-----------|---------------|---------|
| **Model** | YOLOv8n (`yolov8n.pt`) | Lightweight, fast training |
| **Dataset** | COCO128 (`coco128.yaml`) | Small dataset for quick tests |
| **Epochs** | 2 | Balance between speed & results |
| **Device** | CPU | Universal compatibility |
| **Batch Size** | 2 | Memory efficient |
| **Image Size** | 640 | Standard YOLO resolution |

### ğŸ“ Expected Test Artifacts

```bash
runs/
â”œâ”€â”€ detect/
â”‚   â”œâ”€â”€ train/          # Training outputs
â”‚   â”‚   â”œâ”€â”€ weights/
â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt      # Best model weights
â”‚   â”‚   â”‚   â””â”€â”€ last.pt      # Latest checkpoint
â”‚   â”‚   â”œâ”€â”€ results.csv      # Training metrics
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â””â”€â”€ results.png
â”‚   â”œâ”€â”€ val/            # Validation outputs
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”œâ”€â”€ F1_curve.png
â”‚   â”‚   â”œâ”€â”€ PR_curve.png
â”‚   â”‚   â””â”€â”€ results.csv
â”‚   â””â”€â”€ predict/        # Prediction outputs
â”‚       â”œâ”€â”€ bus.jpg     # Annotated predictions
â”‚       â””â”€â”€ labels/     # YOLO format labels
â””â”€â”€ export/
    â””â”€â”€ yolov8n.onnx    # Exported ONNX model
```

### ğŸª Performance Benchmarks

| Operation | Expected Duration | Memory Usage | Success Criteria |
|-----------|------------------|--------------|------------------|
| **Health Check** | < 1s | < 10MB | HTTP 200, healthy status |
| **Training (2 epochs)** | 2-4 min | 1-2GB | mAP50 > 0.3, loss decreasing |
| **Validation** | 30-60s | 500MB | Metrics generated, CSV created |
| **Prediction** | 5-15s | 200MB | Objects detected, images saved |
| **Export** | 10-30s | 300MB | ONNX model created, < 20MB |

## ğŸ“Š Test Results & Analysis

### âœ… Success Indicators

**ğŸ¯ Training Success:**
```python
âœ… return_code == 0
âœ… success == True
âœ… artifacts contain ['best.pt', 'last.pt', 'results.csv']
âœ… metrics.mAP50 > 0.30
âœ… training_time < 300 seconds
```

**ğŸ” Validation Success:**
```python
âœ… return_code == 0
âœ… metrics contain ['mAP50', 'mAP50-95', 'precision', 'recall']
âœ… confusion_matrix.png exists
âœ… validation_time < 60 seconds
```

**ğŸ¯ Prediction Success:**
```python
âœ… return_code == 0
âœ… predictions saved to runs/detect/predict/
âœ… objects detected with confidence > 0.25
âœ… inference_time < 15 seconds
```

### ğŸ“ˆ Example Test Output

```bash
$ python run_tests.py

ğŸ§ª Starting Ultralytics MCP Server Test Suite
===============================================

ğŸ¥ Health Check                               âœ… PASSED (0.2s)
ğŸ”§ Environment Setup                          âœ… PASSED (1.1s)
ğŸš‚ Training YOLOv8n (2 epochs)                âœ… PASSED (187.3s)
   â”œâ”€â”€ mAP50: 0.542
   â”œâ”€â”€ mAP50-95: 0.351
   â”œâ”€â”€ Best weights: runs/detect/train/weights/best.pt
   â””â”€â”€ Training time: 3:07

ğŸ” Model Validation                           âœ… PASSED (42.1s)
   â”œâ”€â”€ Precision: 0.721
   â”œâ”€â”€ Recall: 0.648
   â””â”€â”€ F1-Score: 0.682

ğŸ¯ Prediction Testing                         âœ… PASSED (8.4s)
   â”œâ”€â”€ Objects detected: 3
   â”œâ”€â”€ Confidence threshold: 0.25
   â””â”€â”€ Inference time: 127ms

ğŸ“¤ Model Export (ONNX)                        âœ… PASSED (16.7s)
   â”œâ”€â”€ Export format: ONNX
   â”œâ”€â”€ Model size: 13.4MB
   â””â”€â”€ Export time: 16.7s

ğŸ§¹ Cleanup & Verification                     âœ… PASSED (2.1s)

================================================
ğŸ“Š SUMMARY: 6/6 tests passed âœ…
â±ï¸  Total time: 4:17
ğŸ’¾ Artifacts: 23 files generated
ğŸ¯ Coverage: 95.3%
```

## ğŸ› Troubleshooting Guide

### ğŸš¨ Common Issues & Solutions

#### **âŒ Environment Issues**
```bash
# Issue: ModuleNotFoundError
âŒ ModuleNotFoundError: No module named 'ultralytics'

# Solution: Activate correct environment
âœ… conda activate ultra-dev
âœ… pip install -r requirements.txt
```

#### **ğŸŒ Download Failures**
```bash
# Issue: Model/dataset download fails
âŒ URLError: <urlopen error [Errno 11001] getaddrinfo failed>

# Solution: Check internet connection and proxy
âœ… curl -I https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt
âœ… export HTTPS_PROXY=your-proxy-server:port
```

#### **ğŸ’¾ Disk Space Issues**
```bash
# Issue: Insufficient disk space
âŒ OSError: [Errno 28] No space left on device

# Solution: Clean up previous runs
âœ… rm -rf runs/detect/train*
âœ… df -h  # Check available space
```

#### **â° Timeout Issues**
```bash
# Issue: Tests timeout
âŒ FAILED tests/test_flow.py::test_training - TimeoutError

# Solution: Increase timeout or reduce epochs
âœ… pytest tests/ --timeout=7200  # 2 hour timeout
âœ… Edit test_flow.py: epochs=1    # Reduce training time
```

### ğŸ”§ Debug Commands

```bash
# Verbose output with stack traces
python -m pytest tests/test_flow.py -v -s --tb=long

# Debug specific test with prints
python -m pytest tests/test_flow.py::test_training -v -s --capture=no

# Check API server status
curl -s http://localhost:8000/health | jq

# Monitor resource usage during tests
htop  # or top on macOS/Linux
```

## ğŸ”„ CI/CD Integration

### ğŸ—ï¸ GitHub Actions Configuration

```yaml
# .github/workflows/test.yml
name: Test Suite
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Conda Environment
        uses: conda-incubator/setup-miniconda@v2
        with:
          environment-file: environment.yml
          activate-environment: ultra-dev
          
      - name: Run Test Suite
        run: |
          conda activate ultra-dev
          python -m pytest tests/ --cov=app --cov-report=xml
          
      - name: Upload Coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
```

### ğŸ³ Docker Testing

```bash
# Build test image
docker build -t ultralytics-mcp-test -f Dockerfile.test .

# Run tests in container
docker run --rm ultralytics-mcp-test

# Run with volume for artifacts
docker run --rm -v $(pwd)/test-results:/app/runs ultralytics-mcp-test
```

## ğŸ“š Advanced Testing Patterns

### ğŸ­ Parameterized Tests

```python
import pytest

@pytest.mark.parametrize("model,epochs,expected_map", [
    ("yolov8n.pt", 2, 0.30),
    ("yolov8s.pt", 1, 0.25),
    ("yolov8m.pt", 1, 0.35),
])
def test_training_models(model, epochs, expected_map):
    result = train_model(model=model, epochs=epochs)
    assert result.metrics.mAP50 > expected_map
```

### ğŸ§ª Fixture Usage

```python
@pytest.fixture(scope="session")
def trained_model():
    """Provides a trained model for multiple tests."""
    result = train_model(model="yolov8n.pt", epochs=2)
    yield result.artifacts[0]  # best.pt path
    cleanup_artifacts()

def test_validation_with_trained_model(trained_model):
    result = validate_model(model=trained_model)
    assert result.success
```

### ğŸ“Š Performance Testing

```python
def test_training_performance():
    start_time = time.time()
    result = train_model(model="yolov8n.pt", epochs=1)
    duration = time.time() - start_time
    
    assert result.success
    assert duration < 120  # Must complete within 2 minutes
    assert result.metrics.mAP50 > 0.20
```

## ğŸ“„ License & Contributing

### ğŸ“ Test Guidelines

When contributing new tests:

1. **ğŸ¯ Test Naming**: Use descriptive names (`test_training_with_custom_dataset`)
2. **ğŸ“ Documentation**: Add docstrings explaining test purpose
3. **âš¡ Performance**: Keep individual tests under 5 minutes
4. **ğŸ§¹ Cleanup**: Always clean up generated artifacts
5. **ğŸ“Š Assertions**: Use meaningful assertions with clear error messages

### ğŸ¤ Contributing Tests

```bash
# 1. Create test branch
git checkout -b test/new-feature-tests

# 2. Add tests with documentation
# tests/test_new_feature.py

# 3. Run test suite
python -m pytest tests/ -v

# 4. Submit PR with test results
git add tests/test_new_feature.py
git commit -m "Add comprehensive tests for new feature"
git push origin test/new-feature-tests
```

---

<div align="center">

### ğŸ† **Quality Through Comprehensive Testing**

*Ensuring reliability and performance across all YOLO operations*

**[ğŸ§ª Run Tests](run_tests.py)** | **[ğŸ“Š View Coverage](htmlcov/index.html)** | **[ğŸ› Report Issues](https://github.com/your-org/ultralytics-mcp-server/issues)**

</div>
