import pytest
import json
import os
import shutil
from pathlib import Path
from fastapi.testclient import TestClient
from app.main import app

# Create test client
client = TestClient(app)

# Test configuration
TEST_MODEL = "yolov8n.pt"
TEST_DATA = "coco128.yaml"
TEST_SOURCE = "https://ultralytics.com/images/bus.jpg"
RUNS_DIR = Path("runs")


@pytest.fixture(scope="module", autouse=True)
def setup_and_teardown():
    """Setup and teardown for test module"""
    # Setup: Clean runs directory before tests
    if RUNS_DIR.exists():
        shutil.rmtree(RUNS_DIR)
    
    yield
    
    # Teardown: Optional - keep runs for inspection
    # if RUNS_DIR.exists():
    #     shutil.rmtree(RUNS_DIR)


@pytest.fixture
def train_request_payload():
    """Training request payload for 1 epoch on coco128"""
    return {
        "model": TEST_MODEL,
        "data": TEST_DATA,
        "epochs": 1,
        "imgsz": 640,
        "batch": 2,  # Small batch for faster testing
        "device": "cpu",
        "project": "runs/detect",
        "name": "train",
        "exist_ok": True,
        "verbose": False,
        "cache": False,  # Disable caching for consistent tests
        "save_period": -1,  # Don't save intermediate checkpoints
        "extra_args": {
            "patience": 1  # Early stopping - removed save=False to allow model saving
        }
    }


@pytest.fixture
def val_request_payload():
    """Validation request payload"""
    return {
        "model": "runs/detect/train/weights/best.pt",  # Use trained model
        "data": TEST_DATA,
        "imgsz": 640,
        "batch": 2,
        "device": "cpu",
        "project": "runs/detect",
        "name": "val",
        "exist_ok": True,
        "verbose": False,
        "extra_args": {
            "plots": False  # Disable plots for faster testing
        }
    }


@pytest.fixture
def predict_request_payload():
    """Prediction request payload"""
    return {
        "model": "runs/detect/train/weights/best.pt",  # Use trained model
        "source": TEST_SOURCE,
        "imgsz": 640,
        "device": "cpu",
        "project": "runs/detect",
        "name": "predict",
        "exist_ok": True,
        "verbose": False,
        "save": True,
        "extra_args": {
            "show_labels": False,
            "show_conf": False
        }
    }


class TestUltralyticsFlow:
    """Test the complete Ultralytics workflow"""
    
    def test_health_check(self):
        """Test the health check endpoint"""
        response = client.get("/")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert "version" in data
        assert "timestamp" in data
    
    def test_01_train_model(self, train_request_payload):
        """Test training a model for 1 epoch"""
        print("\nüöÄ Starting training test...")
        
        response = client.post("/train", json=train_request_payload)
        
        # Assert successful response
        assert response.status_code == 200, f"Training failed with status {response.status_code}"
        
        data = response.json()
        
        # Debug: Print the actual response
        print(f"Training response: {data}")
        print(f"Command executed: {data.get('command', 'N/A')}")
        print(f"Return code: {data.get('return_code', 'N/A')}")
        print(f"Success: {data.get('success', 'N/A')}")
        if data.get('stderr'):
            print(f"STDERR: {data['stderr']}")
        if data.get('stdout'):
            print(f"STDOUT (last 500 chars): {data['stdout'][-500:]}")
        
        # Validate response structure
        assert "run_id" in data
        assert "command" in data
        assert "return_code" in data
        assert "success" in data
        assert "artifacts" in data
        assert "metrics" in data
        
        # Check if training was successful
        assert data["success"] is True, f"Training failed: {data.get('stderr', '')}"
        assert data["return_code"] == 0, f"Non-zero return code: {data['return_code']}"
        
        # Check for training artifacts
        train_dir = Path("runs/detect/train")
        assert train_dir.exists(), "Training directory was not created"
        
        # List all files in train directory for debugging
        train_files = list(train_dir.rglob("*"))
        print(f"Files in training directory: {train_files}")
        
        # Check for essential training files - be more flexible about weights
        weights_dir = train_dir / "weights"
        if not weights_dir.exists():
            # Look for weight files anywhere in the train directory
            weight_files = list(train_dir.rglob("*.pt"))
            print(f"Weight files found: {weight_files}")
            assert len(weight_files) > 0, "No weight files (.pt) were found in training directory"
        else:
            # Check for standard weight files
            best_weights = weights_dir / "best.pt"
            last_weights = weights_dir / "last.pt"
            assert best_weights.exists() or last_weights.exists(), "No model weights were saved"
        
        # Check for results
        results_csv = train_dir / "results.csv"
        assert results_csv.exists(), "Results CSV was not created"
        
        print(f"‚úÖ Training completed successfully!")
        print(f"   - Command: {data['command']}")
        print(f"   - Artifacts: {len(data['artifacts'])} files created")
        
        # Store training results for next tests
        TestUltralyticsFlow.train_results = data
    
    def test_02_validate_model(self, val_request_payload):
        """Test validating the trained model"""
        print("\nüîç Starting validation test...")
        
        # Ensure training was completed first
        assert hasattr(TestUltralyticsFlow, 'train_results'), "Training must complete before validation"
        
        # Check if trained model exists
        model_path = Path(val_request_payload["model"])
        if not model_path.exists():
            # Fallback to last.pt if best.pt doesn't exist
            val_request_payload["model"] = "runs/detect/train/weights/last.pt"
            model_path = Path(val_request_payload["model"])
        
        assert model_path.exists(), f"Trained model not found at {model_path}"
        
        response = client.post("/val", json=val_request_payload)
        
        # Assert successful response
        assert response.status_code == 200, f"Validation failed with status {response.status_code}"
        
        data = response.json()
        
        # Validate response structure
        assert "run_id" in data
        assert "success" in data
        assert data["success"] is True, f"Validation failed: {data.get('stderr', '')}"
        assert data["return_code"] == 0, f"Non-zero return code: {data['return_code']}"
        
        # Check for validation artifacts
        val_dir = Path("runs/detect/val")
        assert val_dir.exists(), "Validation directory was not created"
        
        print(f"‚úÖ Validation completed successfully!")
        print(f"   - Command: {data['command']}")
        
        # Store validation results
        TestUltralyticsFlow.val_results = data
    
    def test_03_predict_with_model(self, predict_request_payload):
        """Test making predictions with the trained model"""
        print("\nüéØ Starting prediction test...")
        
        # Ensure training was completed first
        assert hasattr(TestUltralyticsFlow, 'train_results'), "Training must complete before prediction"
        
        # Check if trained model exists
        model_path = Path(predict_request_payload["model"])
        if not model_path.exists():
            # Fallback to last.pt if best.pt doesn't exist
            predict_request_payload["model"] = "runs/detect/train/weights/last.pt"
            model_path = Path(predict_request_payload["model"])
        
        assert model_path.exists(), f"Trained model not found at {model_path}"
        
        response = client.post("/predict", json=predict_request_payload)
        
        # Assert successful response
        assert response.status_code == 200, f"Prediction failed with status {response.status_code}"
        
        data = response.json()
        
        # Validate response structure
        assert "run_id" in data
        assert "success" in data
        assert data["success"] is True, f"Prediction failed: {data.get('stderr', '')}"
        assert data["return_code"] == 0, f"Non-zero return code: {data['return_code']}"
        
        # Check for prediction artifacts
        predict_dir = Path("runs/detect/predict")
        assert predict_dir.exists(), "Prediction directory was not created"
        
        # Check for prediction results
        prediction_files = list(predict_dir.glob("*.jpg")) + list(predict_dir.glob("*.png"))
        assert len(prediction_files) > 0, "No prediction images were saved"
        
        print(f"‚úÖ Prediction completed successfully!")
        print(f"   - Command: {data['command']}")
        print(f"   - Prediction files: {len(prediction_files)}")
        
        # Store prediction results
        TestUltralyticsFlow.predict_results = data
    
    def test_04_check_results_json(self):
        """Test that results.json exists and contains valid data"""
        print("\nüìä Checking results.json...")
        
        # Check training results.json
        train_results_path = Path("runs/detect/train/results.csv")
        assert train_results_path.exists(), "Training results.csv not found"
        
        # Try to find any JSON results files
        json_files = []
        for results_dir in ["runs/detect/train", "runs/detect/val", "runs/detect/predict"]:
            if Path(results_dir).exists():
                json_files.extend(list(Path(results_dir).glob("*.json")))
        
        print(f"   - Found {len(json_files)} JSON result files")
        
        # Validate at least some results exist
        csv_files = list(Path("runs/detect").glob("**/results.csv"))
        assert len(csv_files) > 0, "No results.csv files found"
        
        # Read and validate training results CSV
        with open(train_results_path, 'r') as f:
            csv_content = f.read()
            assert len(csv_content.strip()) > 0, "Results CSV is empty"
            assert "epoch" in csv_content.lower(), "Results CSV doesn't contain epoch data"
        
        print(f"‚úÖ Results validation completed!")
        print(f"   - CSV files: {len(csv_files)}")
        print(f"   - JSON files: {len(json_files)}")
    
    def test_05_workflow_summary(self):
        """Print a summary of the complete workflow test"""
        print("\nüìã Workflow Test Summary:")
        print("=" * 50)
        
        if hasattr(TestUltralyticsFlow, 'train_results'):
            print(f"‚úÖ Training: {TestUltralyticsFlow.train_results['success']}")
            print(f"   - Artifacts: {len(TestUltralyticsFlow.train_results['artifacts'])}")
        
        if hasattr(TestUltralyticsFlow, 'val_results'):
            print(f"‚úÖ Validation: {TestUltralyticsFlow.val_results['success']}")
        
        if hasattr(TestUltralyticsFlow, 'predict_results'):
            print(f"‚úÖ Prediction: {TestUltralyticsFlow.predict_results['success']}")
        
        # Check final directory structure
        if RUNS_DIR.exists():
            total_files = len(list(RUNS_DIR.rglob("*")))
            print(f"üìÅ Total files created: {total_files}")
            
            for subdir in ["train", "val", "predict"]:
                subdir_path = RUNS_DIR / "detect" / subdir
                if subdir_path.exists():
                    file_count = len(list(subdir_path.rglob("*")))
                    print(f"   - {subdir}/: {file_count} files")
        
        print("=" * 50)
        print("üéâ Complete workflow test passed!")


# Additional individual endpoint tests
class TestIndividualEndpoints:
    """Test individual endpoints with various scenarios"""
    
    def test_train_endpoint_validation(self):
        """Test training endpoint input validation"""
        # Test missing required field
        invalid_payload = {"epochs": 1}
        response = client.post("/train", json=invalid_payload)
        assert response.status_code == 422  # Validation error
    
    def test_predict_endpoint_validation(self):
        """Test prediction endpoint input validation"""
        # Test missing required fields
        invalid_payload = {"imgsz": 640}
        response = client.post("/predict", json=invalid_payload)
        assert response.status_code == 422  # Validation error
    
    def test_export_endpoint(self):
        """Test export endpoint with a basic model"""
        export_payload = {
            "model": TEST_MODEL,
            "format": "onnx",
            "imgsz": 640,
            "device": "cpu",
            "extra_args": {
                "dynamic": False,
                "simplify": True
            }
        }
        
        response = client.post("/export", json=export_payload)
        # Note: This might fail if model download/export fails, so we're lenient
        assert response.status_code in [200, 500]  # Allow for download failures in CI


if __name__ == "__main__":
    # Run with: python -m pytest tests/test_flow.py -v -s
    pytest.main([__file__, "-v", "-s"])
